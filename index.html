<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Do DALL-E and Flamingo Understand Each Other?">
  <meta name="keywords" content="Multimodal Generative Models, Text-to-Image, Image Captioning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DALLE Flamingo</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>  
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <!-- <script>

    function updateInteractive() {
      var task = document.getElementById("interative-menu").value;

      console.log("interactive", task)

      // if task does not contain the string "towel"
      if (task.indexOf("towel") == -1) {
        var video = document.getElementById("interactive-video");
        video.src = "media/videos/" + 
                    task + 
                    ".mp4"
        video.play();

        var html = document.getElementById("interactive-html-1");
        html.src = "media/interactive/" + 
                    task + 
                    ".html"

        // hide the second iframe container
        var iframeContainer2 = document.getElementById("second-iframe-container");
        iframeContainer2.style.display = "none";
      } else {
        var video = document.getElementById("interactive-video");
        video.src = "media/videos/hang-towel.mp4"
        video.play();

        var html1 = document.getElementById("interactive-html-1");
        html1.src = "media/interactive/hang-towel-1.html"

        // show and set the source for the second iframe
        var html2 = document.getElementById("interactive-html-2");
        html2.src = "media/interactive/hang-towel-2.html"

        // show the second iframe container
        var iframeContainer2 = document.getElementById("second-iframe-container");
        iframeContainer2.style.display = "block";
      }
    }



  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Do DALL-E and Flamingo Understand Each Other?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://hangligit.github.io/">Hang Li*</a><sup>1, 2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://jindonggu.github.io/">Jindong Gu*</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/rajat-koner/">Rajat Koner</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/sahandsharifzadeh/?locale=de_DE">Sahand Sharifzadeh</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>LMU Munich,</span>
            <span class="author-block"><sup>2</sup>Siemens AG,</span>
            <span class="author-block"><sup>3</sup>University of Oxford</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2212.12249"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2212.12249"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://dalleflamingo.github.io"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://dalleflamingo.github.io"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming Soon)</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="rows">
          <div style="text-align: center;">
            <img src="images/Page1.png" class="interpolation-image" style="width: 100%; height: auto;"/>
          </div>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dperact">DALLE(SD) and FLAMINGO(BLIP)</span> Talks to Each Other. <br> We explore two types of large-scale multimodal generative models, image-to-text and text-to-image. The image-to-text model generates abstract descriptions of an image, whereas the text-to-image model decodes the text into low-level visual pixel features. These two models are closely related but their relationship is little understood. In this work, we study if large multimodal generative models understand each other. Specifically, if Flamingo describes an image in text, can DALLE reconstruct an image similar to the input image from the text?
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/take-out-the-toaster-and-put-it-on-the-wooden-plate.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/close-top-drawer-dist.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/sort-trash-to-tray-dist.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
           <video poster="" autoplay muted loop height="100%">
             <source src="media/videos/open-bottle.mp4"
                     type="video/mp4">
           </video>
         </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/turn-on-the-lamp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/sweep-the-trash-into-the-dustpan.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/set-up-the-utensils-for-my-pasta.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/unplug-charger.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/get-napkin.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/hang-towel.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/measure-apple.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  VoxPoser can <b>zero-shot synthesize</b> trajectories for real-world manipulation tasks with an <b>open-set</b> of free-form language instructions and an <b>open-set</b> of objects.
</h2> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The field of multimodal research focusing on the comprehension and creation of both images and text has witnessed significant strides. This progress is exemplified by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example. An interesting question worth exploring in this domain is whether Flamingo and DALL-E understand each other. To study this question, we propose a reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image. We argue that these models understand each other if the generated image is similar to the given image. Specifically, we study the relationship between the quality of the image reconstruction and that of the text generation. We find that an optimal description of an image is one that gives rise to a generated image similar to the original one. The finding motivates us to propose a unified framework to finetune the text-to-image and image-to-text models. Concretely, the reconstruction part forms a regularization loss to guide the tuning of the models. Extensive experiments on multiple datasets with different image captioning and image generation models validate our findings and demonstrate the effectiveness of our proposed unified framework. As DALL-E and Flamingo are not publicly available, we use Stable Diffusion and BLIP in the remaining work.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  <!-- Paper video. -->
  <!-- <br>
  <br>

  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/Yvn4eR05A3M"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->

</section>


<!-- 
<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Execution under Disturbances</h2>
      
      <p class="content has-text-justified">
        Because the language model output stays the same throughout the task, we can cache its output and re-evaluate the generated code using closed-loop visual feedback, which enables fast replanning using MPC. This enables VoxPoser to be robust to online disturbances.
      </p>
      
      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            controls
            muted
            autoplay
            loop
            width="99%">
            <source src="media/videos/sort-trash-to-tray-dist.mp4" 
            type="video/mp4">
          </video>
          <p style="text-align:center">
            "Sort the paper trash into the blue tray."
          </p>
        </div>
    
        <div class="column has-text-centered">
          <video id="dist2"
            controls
            muted
            autoplay
            loop
            width="99%">
            <source src="media/videos/close-top-drawer-dist.mp4" 
            type="video/mp4">
          </video>
          <p style="text-align:center">
            "Close the top drawer."
          </p>
        </div>
  </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Findings</h2>
      <div style="text-align: center;">
        <img src="images/Page3.png" class="interpolation-image" style="width: 120%; height: auto;"/>
      </div>
      <br>
      <p class="content has-text-justified">
        Qualitative examples of reconstruction tasks. The left side shows image reconstruction for the given input image of a bird. Two samples are shown with their generated images, ranked by the image similarity. The right side shows text reconstruction whereas the first sample gives a high-quality image as well as high-quality text reconstruction.
      </p>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Method</h2>
      <div style="text-align: center;">
        <img src="images/Page2.png" class="interpolation-image" style="width: 120%; height: auto;"/>
      </div>
      <br>
      <p class="content has-text-justified">
        Illustration of our proposed finetuning framework. We propose a reconstruction pipeline starting from an image (left) and a reconstruction pipeline starting from the text (right). Input images and text shown at the bottom (dashed boxes) are only used during training and will be dropped during inference. For illustration proposes, we omit the VAE in SD without changing the principles. Causal masking is a technique for language modeling during training. The diffusion step denotes adding noise to the input image.
      </p>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2022dall,
      title={Do DALL-E and Flamingo Understand Each Other?},
      author={Li, Hang and Gu, Jindong and Koner, Rajat and Sharifzadeh, Sahand and Tresp, Volker},
      journal={ICCV},
      year={2023}
    }</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://voxposer.github.io/">VoxPoser</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
